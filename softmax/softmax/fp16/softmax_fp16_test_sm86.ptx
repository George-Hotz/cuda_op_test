//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36037853
// Cuda compilation tools, release 12.9, V12.9.86
// Based on NVVM 7.0.1
//

.version 8.8
.target sm_86
.address_size 64

	// .globl	_Z26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_ii
// _ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared has been demoted

.visible .entry _Z26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_ii(
	.param .u64 _Z26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_ii_param_0,
	.param .u64 _Z26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_ii_param_1,
	.param .u32 _Z26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_ii_param_2,
	.param .u32 _Z26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_ii_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<150>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<130>;
	.reg .b64 	%rd<26>;
	// demoted variable
	.shared .align 2 .b8 _ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared[32];

	ld.param.u64 	%rd4, [_Z26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_ii_param_0];
	ld.param.u64 	%rd5, [_Z26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_ii_param_1];
	ld.param.u32 	%r15, [_Z26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_ii_param_3];
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r129, %tid.x;
	and.b32  	%r2, %r129, 31;
	mov.u32 	%r16, %ctaid.x;
	mul.lo.s32 	%r17, %r16, %r15;
	cvt.s64.s32 	%rd3, %r17;
	mov.f32 	%f5, 0fC77FE000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs146, %f5;}

	// end inline asm
	setp.ge.s32 	%p1, %r129, %r15;
	@%p1 bra 	$L__BB0_3;

	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r126, %r129;

$L__BB0_2:
	cvt.s64.s32 	%rd6, %r126;
	add.s64 	%rd7, %rd6, %rd3;
	shl.b64 	%rd8, %rd7, 1;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.u16 	%rs20, [%rd9];
	// begin inline asm
	{max.f16 %rs146,%rs146,%rs20;
}
	// end inline asm
	add.s32 	%r126, %r126, %r3;
	setp.lt.s32 	%p2, %r126, %r15;
	@%p2 bra 	$L__BB0_2;

$L__BB0_3:
	// begin inline asm
	{  mov.b32 %r18, {%rs146,%rs146};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r19, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r58, %r19, 8;
	mov.u32 	%r30, 8;
	add.s32 	%r59, %r58, -8192;
	or.b32  	%r23, %r59, 31;
	mov.u32 	%r22, 16;
	mov.u32 	%r56, -1;
	// begin inline asm
	{shfl.sync.bfly.b32 %r20,%r18,%r22,%r23,%r56;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r20;
 mov.b16 %rs23, low;}
	// end inline asm
	// begin inline asm
	{max.f16 %rs24,%rs146,%rs23;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r26, {%rs24,%rs24};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r27, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r60, %r27, 8;
	add.s32 	%r61, %r60, -8192;
	or.b32  	%r31, %r61, 31;
	// begin inline asm
	{shfl.sync.bfly.b32 %r28,%r26,%r30,%r31,%r56;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r28;
 mov.b16 %rs29, low;}
	// end inline asm
	// begin inline asm
	{max.f16 %rs30,%rs24,%rs29;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r34, {%rs30,%rs30};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r35, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r62, %r35, 8;
	add.s32 	%r63, %r62, -8192;
	or.b32  	%r39, %r63, 31;
	mov.u32 	%r38, 4;
	// begin inline asm
	{shfl.sync.bfly.b32 %r36,%r34,%r38,%r39,%r56;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r36;
 mov.b16 %rs35, low;}
	// end inline asm
	// begin inline asm
	{max.f16 %rs36,%rs30,%rs35;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r42, {%rs36,%rs36};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r43, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r64, %r43, 8;
	add.s32 	%r65, %r64, -8192;
	or.b32  	%r47, %r65, 31;
	mov.u32 	%r46, 2;
	// begin inline asm
	{shfl.sync.bfly.b32 %r44,%r42,%r46,%r47,%r56;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r44;
 mov.b16 %rs41, low;}
	// end inline asm
	// begin inline asm
	{max.f16 %rs42,%rs36,%rs41;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r50, {%rs42,%rs42};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r51, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r66, %r51, 8;
	add.s32 	%r67, %r66, -8192;
	or.b32  	%r55, %r67, 31;
	mov.u32 	%r54, 1;
	// begin inline asm
	{shfl.sync.bfly.b32 %r52,%r50,%r54,%r55,%r56;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r52;
 mov.b16 %rs47, low;}
	// end inline asm
	// begin inline asm
	{max.f16 %rs48,%rs42,%rs47;
}
	// end inline asm
	shr.u32 	%r68, %r129, 4;
	and.b32  	%r69, %r68, 268435454;
	mov.u32 	%r70, _ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared;
	add.s32 	%r6, %r70, %r69;
	setp.ne.s32 	%p3, %r2, 0;
	@%p3 bra 	$L__BB0_5;

	st.shared.u16 	[%r6], %rs48;

$L__BB0_5:
	bar.sync 	0;
	setp.ne.s32 	%p4, %r129, 0;
	@%p4 bra 	$L__BB0_7;

	ld.shared.u16 	%rs52, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared];
	ld.shared.u16 	%rs53, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+2];
	// begin inline asm
	{max.f16 %rs51,%rs52,%rs53;
}
	// end inline asm
	ld.shared.u16 	%rs56, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+4];
	// begin inline asm
	{max.f16 %rs54,%rs51,%rs56;
}
	// end inline asm
	ld.shared.u16 	%rs59, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+6];
	// begin inline asm
	{max.f16 %rs57,%rs54,%rs59;
}
	// end inline asm
	ld.shared.u16 	%rs62, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+8];
	// begin inline asm
	{max.f16 %rs60,%rs57,%rs62;
}
	// end inline asm
	ld.shared.u16 	%rs65, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+10];
	// begin inline asm
	{max.f16 %rs63,%rs60,%rs65;
}
	// end inline asm
	ld.shared.u16 	%rs68, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+12];
	// begin inline asm
	{max.f16 %rs66,%rs63,%rs68;
}
	// end inline asm
	ld.shared.u16 	%rs71, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+14];
	// begin inline asm
	{max.f16 %rs69,%rs66,%rs71;
}
	// end inline asm
	st.shared.u16 	[_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared], %rs69;

$L__BB0_7:
	bar.sync 	0;
	@%p1 bra 	$L__BB0_10;

	ld.shared.u16 	%rs6, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared];
	mov.u32 	%r7, %ntid.x;
	cvt.u32.u64 	%r71, %rd3;
	mov.u32 	%r127, %r129;

$L__BB0_9:
	cvt.s64.s32 	%rd10, %r127;
	add.s64 	%rd11, %rd10, %rd3;
	shl.b64 	%rd12, %rd11, 1;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.u16 	%rs73, [%rd13];
	// begin inline asm
	{sub.f16 %rs72,%rs73,%rs6;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs72;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs75,r;           
}
	// end inline asm
	add.s32 	%r72, %r127, %r71;
	mul.wide.s32 	%rd14, %r72, 2;
	add.s64 	%rd15, %rd2, %rd14;
	st.global.u16 	[%rd15], %rs75;
	add.s32 	%r127, %r127, %r7;
	setp.lt.s32 	%p6, %r127, %r15;
	@%p6 bra 	$L__BB0_9;

$L__BB0_10:
	mov.f32 	%f6, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs148, %f6;}

	// end inline asm
	@%p1 bra 	$L__BB0_13;

	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r128, %r129;

$L__BB0_12:
	cvt.s64.s32 	%rd16, %r128;
	add.s64 	%rd17, %rd16, %rd3;
	shl.b64 	%rd18, %rd17, 1;
	add.s64 	%rd19, %rd2, %rd18;
	ld.global.u16 	%rs80, [%rd19];
	// begin inline asm
	{add.f16 %rs148,%rs148,%rs80;
}
	// end inline asm
	add.s32 	%r128, %r128, %r10;
	setp.lt.s32 	%p8, %r128, %r15;
	@%p8 bra 	$L__BB0_12;

$L__BB0_13:
	// begin inline asm
	{  mov.b32 %r73, {%rs148,%rs148};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r74, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r113, %r74, 8;
	mov.u32 	%r85, 8;
	add.s32 	%r114, %r113, -8192;
	or.b32  	%r78, %r114, 31;
	mov.u32 	%r77, 16;
	mov.u32 	%r111, -1;
	// begin inline asm
	{shfl.sync.bfly.b32 %r75,%r73,%r77,%r78,%r111;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r75;
 mov.b16 %rs83, low;}
	// end inline asm
	// begin inline asm
	{add.f16 %rs84,%rs148,%rs83;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r81, {%rs84,%rs84};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r82, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r115, %r82, 8;
	add.s32 	%r116, %r115, -8192;
	or.b32  	%r86, %r116, 31;
	// begin inline asm
	{shfl.sync.bfly.b32 %r83,%r81,%r85,%r86,%r111;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r83;
 mov.b16 %rs89, low;}
	// end inline asm
	// begin inline asm
	{add.f16 %rs90,%rs84,%rs89;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r89, {%rs90,%rs90};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r90, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r117, %r90, 8;
	add.s32 	%r118, %r117, -8192;
	or.b32  	%r94, %r118, 31;
	mov.u32 	%r93, 4;
	// begin inline asm
	{shfl.sync.bfly.b32 %r91,%r89,%r93,%r94,%r111;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r91;
 mov.b16 %rs95, low;}
	// end inline asm
	// begin inline asm
	{add.f16 %rs96,%rs90,%rs95;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r97, {%rs96,%rs96};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r98, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r119, %r98, 8;
	add.s32 	%r120, %r119, -8192;
	or.b32  	%r102, %r120, 31;
	mov.u32 	%r101, 2;
	// begin inline asm
	{shfl.sync.bfly.b32 %r99,%r97,%r101,%r102,%r111;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r99;
 mov.b16 %rs101, low;}
	// end inline asm
	// begin inline asm
	{add.f16 %rs102,%rs96,%rs101;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r105, {%rs102,%rs102};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r106, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r121, %r106, 8;
	add.s32 	%r122, %r121, -8192;
	or.b32  	%r110, %r122, 31;
	mov.u32 	%r109, 1;
	// begin inline asm
	{shfl.sync.bfly.b32 %r107,%r105,%r109,%r110,%r111;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r107;
 mov.b16 %rs107, low;}
	// end inline asm
	// begin inline asm
	{add.f16 %rs108,%rs102,%rs107;
}
	// end inline asm
	@%p3 bra 	$L__BB0_15;

	st.shared.u16 	[%r6+16], %rs108;

$L__BB0_15:
	bar.sync 	0;
	@%p4 bra 	$L__BB0_17;

	ld.shared.u16 	%rs112, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+16];
	ld.shared.u16 	%rs113, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+18];
	// begin inline asm
	{add.f16 %rs111,%rs112,%rs113;
}
	// end inline asm
	ld.shared.u16 	%rs116, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+20];
	// begin inline asm
	{add.f16 %rs114,%rs111,%rs116;
}
	// end inline asm
	ld.shared.u16 	%rs119, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+22];
	// begin inline asm
	{add.f16 %rs117,%rs114,%rs119;
}
	// end inline asm
	ld.shared.u16 	%rs122, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+24];
	// begin inline asm
	{add.f16 %rs120,%rs117,%rs122;
}
	// end inline asm
	ld.shared.u16 	%rs125, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+26];
	// begin inline asm
	{add.f16 %rs123,%rs120,%rs125;
}
	// end inline asm
	ld.shared.u16 	%rs128, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+28];
	// begin inline asm
	{add.f16 %rs126,%rs123,%rs128;
}
	// end inline asm
	ld.shared.u16 	%rs131, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+30];
	// begin inline asm
	{add.f16 %rs129,%rs126,%rs131;
}
	// end inline asm
	st.shared.u16 	[_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+16], %rs129;

$L__BB0_17:
	bar.sync 	0;
	@%p1 bra 	$L__BB0_23;

	ld.shared.u16 	%rs12, [_ZZ26softmax_f16_per_mat_kernelILi256EEvP6__halfPKS0_iiE6shared+16];
	// begin inline asm
	{  cvt.f32.f16 %f8, %rs12;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f9, %f8;
}
	// end inline asm
	neg.ftz.f32 	%f14, %f8;
	cvt.u32.u64 	%r123, %rd3;

$L__BB0_19:
	cvt.s64.s32 	%rd20, %r129;
	add.s64 	%rd21, %rd20, %rd3;
	shl.b64 	%rd22, %rd21, 1;
	add.s64 	%rd23, %rd2, %rd22;
	ld.global.u16 	%rs132, [%rd23];
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs132;}

	// end inline asm
	mul.ftz.f32 	%f11, %f7, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs149, %f11;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs135,%rs149;
}
	// end inline asm
	mov.u16 	%rs139, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs135, %rs139;
  selp.u16 %rs137, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p12, %rs137, 0;
	@%p12 bra 	$L__BB0_22;

	mov.f32 	%f12, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs140, %f12;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs140, %rs135;
  selp.u16 %rs141, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p13, %rs141, 0;
	@%p13 bra 	$L__BB0_22;

	fma.rn.ftz.f32 	%f15, %f14, %f11, %f7;
	fma.rn.ftz.f32 	%f13, %f9, %f15, %f11;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs149, %f13;}

	// end inline asm

$L__BB0_22:
	add.s32 	%r124, %r129, %r123;
	mul.wide.s32 	%rd24, %r124, 2;
	add.s64 	%rd25, %rd2, %rd24;
	st.global.u16 	[%rd25], %rs149;
	mov.u32 	%r125, %ntid.x;
	add.s32 	%r129, %r129, %r125;
	setp.lt.s32 	%p14, %r129, %r15;
	@%p14 bra 	$L__BB0_19;

$L__BB0_23:
	ret;

}

